# DiseÃ±o e ImplementaciÃ³n de un Pipeline ETL para la IntegraciÃ³n y AnÃ¡lisis de Ventas E-Commerce (2023-2025)

**Autor:** Ing. RubÃ©n Soria\
**Docente:** Ing. Anthony Huaccachi\
**EspecializaciÃ³n:** IngenierÃ­a de Datos e Inteligencia Artificial con
Databricks

------------------------------------------------------------------------

## DescripciÃ³n General

Este proyecto tiene como objetivo **implementar un proceso ETL
completo** (Extract, Transform, Load) para consolidar y disponibilizar
la informaciÃ³n de ventas generadas por el canal **e-commerce** entre los
aÃ±os **2023 y 2025**.
El desarrollo se realizÃ³ en **Microsoft Azure**, utilizando
**Databricks** como plataforma principal para la ingenierÃ­a de datos,
siguiendo el **patrÃ³n de arquitectura Medallion** (Bronze, Silver,
Golden).

------------------------------------------------------------------------

## Objetivo del Proyecto

Desarrollar un proceso automatizado que permita:

-   Extraer datos de diversas fuentes transaccionales del e-commerce.
-   Transformar y limpiar los datos para garantizar su integridad y
    consistencia.
-   Cargar los datos normalizados en un **repositorio centralizado**.
-   Disponibilizar la informaciÃ³n para su anÃ¡lisis mediante **Power BI**
    y otras herramientas BI.

### CaracterÃ­sticas del ETL

-   Estructura y normalizaciÃ³n del modelo de datos.
-   DepuraciÃ³n de duplicados y validaciones de calidad.
-   ActualizaciÃ³n periÃ³dica (diaria o semanal).
-   ExposiciÃ³n en base de datos **Azure SQL Database**.

------------------------------------------------------------------------

## Arquitectura del Proyecto


<img width="425" height="233" alt="image" src="https://github.com/user-attachments/assets/2d099b1e-615b-4764-a5b2-a630030e6447" />



El proyecto se implementa Ã­ntegramente en **Azure Cloud**, aprovechando
sus servicios nativos:

  Servicio               Nombre
  ---------------------- -------------------
  Storage Account        `adlsecommerce`
  Access Connector       `ac-ecommerce`
  Container              `metastore-prod`
  Databricks Workspace   `prod-ecommerce`
  Azure SQL Server       `serverecommerce`
  Azure SQL Database     `ecommerce`

### PatrÃ³n Medallion

El flujo de datos sigue las capas: 
1. **Raw / Bronze:** Datos originales
sin procesar.
2. **Silver:** Datos transformados y validados.
3. **Golden:** Datos listos para anÃ¡lisis y consumo BI.

------------------------------------------------------------------------

##  ImplementaciÃ³n TÃ©cnica


<img width="398" height="231" alt="image" src="https://github.com/user-attachments/assets/4af6bc94-e41f-4829-8ae2-346cd74d2202" />



###  ConfiguraciÃ³n en Ambiente Develop

-   CreaciÃ³n del **Storage Account** con contenedores: `metastoredata`,
    `raw`, `bronze`, `silver`, `golden`.
-   Carga de la fuente: `Ecommerce_Sales_Prediction_Dataset.csv`.
-   ConfiguraciÃ³n de **Managed Identity** y **External Location**.
-   CreaciÃ³n de un **Metastore** dedicado para aislar entornos de
    desarrollo y producciÃ³n.

###  CreaciÃ³n de Notebooks

**Dataset fuente:** `Ecommerce_Sales_Prediction_Dataset.csv`
| Columna | DescripciÃ³n |
|---------|-------------|
| `Date` | Fecha de la venta (desde 01-01-2023) |
| `Product_Category` | CategorÃ­a del producto |
| `Price` | Precio del producto |
| `Discount` | Descuento aplicado |
| `Customer_Segment` | Segmento del comprador |
| `Marketing_Spend` | Presupuesto de marketing |
| `Units_Sold` | Unidades vendidas |

ExploraciÃ³n inicial de datos y validaciones de consistencia.

###  Workflow y EjecuciÃ³n

Durante la prueba del workflow, se presentÃ³ un error en la tarea
`task_transform` debido a la configuraciÃ³n ANSI de Spark.
Se resolviÃ³ mediante:

``` python
spark.conf.set("spark.sql.ansi.enabled", "false")
```

Tras la correcciÃ³n, el **workflow finalizÃ³ exitosamente**, generando
catÃ¡logos, esquemas y tablas **Delta Parquet**.

------------------------------------------------------------------------

## CI/CD con GitHub Actions

Se configuraron **secretos y workflows de despliegue automatizado**
mediante **GitHub Actions**.
- Archivo `script.yml` contiene la orquestaciÃ³n del pipeline.
- Cada **Pull Request** genera la ejecuciÃ³n automÃ¡tica del flujo de
integraciÃ³n y despliegue.
- ValidaciÃ³n de parÃ¡metros y entornos (`develop`, `production`).

------------------------------------------------------------------------

##  Despliegue en ProducciÃ³n

-   DuplicaciÃ³n de estructura para mÃºltiples regiones.
-   ConfiguraciÃ³n de parÃ¡metros de entorno en los Jobs.
-   EjecuciÃ³n del pipeline desde **GitHub Actions**, con correcciones
    iterativas en catÃ¡logos y esquemas.
-   ValidaciÃ³n de Ã©xito del workflow y creaciÃ³n de tablas `bronze`,
    `silver`, `golden`.

VisualizaciÃ³n de los archivos **Delta Parquet** en Azure CLI y
confirmaciÃ³n de la data migrada correctamente.

------------------------------------------------------------------------

##  DisponibilizaciÃ³n de Data en Azure Database

Desde el workspace `prod_ecommerce` se creÃ³ un notebook para **cargar la
data Golden** hacia **Azure SQL Database** (`ecommerce`) mediante
DataFrames.
Esta capa se emplea como **fuente principal para el anÃ¡lisis BI**.

------------------------------------------------------------------------

##  Dashboard en Power BI

-   ConexiÃ³n directa con **Azure SQL Database**.
-   ImportaciÃ³n de la tabla `sales_orders_gold`.
-   ValidaciÃ³n de consistencia y exploraciÃ³n inicial de datos.
-   CreaciÃ³n de dashboard con indicadores clave de negocio:
    -   Ventas por categorÃ­a.
    -   Efectividad de campaÃ±as.
    -   SegmentaciÃ³n de clientes.
    -   AnÃ¡lisis de descuentos vs.Â ventas.
 


<img width="523" height="259" alt="image" src="https://github.com/user-attachments/assets/a81dd860-b870-4e22-891b-de9a60bed44f" />




------------------------------------------------------------------------

##  Resultados

-   ETL implementado y validado de extremo a extremo.
-   Proceso CI/CD automatizado.
-   Datos disponibles en Azure SQL Database.
-   Dashboard analÃ­tico operativo en Power BI.
-   Arquitectura modular, escalable y segura basada en Azure +
    Databricks.

------------------------------------------------------------------------

##   TecnologÃ­as Utilizadas

-   **Microsoft Azure**
    -   Azure Storage Account
    -   Azure Databricks
    -   Azure SQL Database
-   **Databricks**
    -   PySpark / SQL
    -   Delta Lake / Unity Catalog
-   **GitHub Actions** (CI/CD)
-   **Power BI**
-   **Python**

------------------------------------------------------------------------

##  Estructura del Proyecto

    /ecommerce/
    â”œâ”€â”€ develop/
    â”‚   â”œâ”€â”€ notebooks/
    â”‚   â”‚   â”œâ”€â”€ extract.ipynb
    â”‚   â”‚   â”œâ”€â”€ transform.ipynb
    â”‚   â”‚   â””â”€â”€ load.ipynb
    â”‚   â””â”€â”€ workflows/
    â”‚       â””â”€â”€ etl_job.json
    â”œâ”€â”€ production/
    â”‚   â”œâ”€â”€ scripts/
    â”‚   â”‚   â””â”€â”€ main/
    â”‚   â””â”€â”€ config/
    â”‚       â””â”€â”€ parameters.json
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ Ecommerce_Sales_Prediction_Dataset.csv
    â””â”€â”€ .github/
        â””â”€â”€ workflows/
            â””â”€â”€ script.yml

------------------------------------------------------------------------

##  Autor

**Ing. RubÃ©n Soria**\
Data Engineer\
ðŸ“§ *\[ruben.soria@outlook.com]*\
ðŸ’¼ *\[LinkedIn: https://www.linkedin.com/in/rubensoria-sys/]*
